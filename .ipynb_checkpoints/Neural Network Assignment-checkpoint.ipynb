{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project: Create a neural network class\n",
    "\n",
    "---\n",
    "\n",
    "Based on previous code examples, develop a neural network class that is able to classify any dataset provided. The class should create objects based on the desired network architecture:\n",
    "\n",
    "1. Number of inputs\n",
    "2. Number of hidden layers\n",
    "3. Number of neurons per layer\n",
    "4. Number of outputs\n",
    "5. Learning rate\n",
    "\n",
    "The class must have the train, and predict functions.\n",
    "\n",
    "Test the neural network class on the datasets provided below: Use the input data to train the network, and then pass new inputs to predict on. Print the expected label and the predicted label for the input you used. Print the accuracy of the training after predicting on different inputs.\n",
    "\n",
    "Use matplotlib to plot the error that the train method generates.\n",
    "\n",
    "**Don't forget to install Keras and tensorflow in your environment!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the needed Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Needed for the mnist data\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, numInputs, numHiddenLayers, numNeuronsPerLayer, numOutputs, alpha):\n",
    "        '''\n",
    "            layers: List of integers which represents the architecture of the network.\n",
    "            alpha:  Learning rate.\n",
    "        '''\n",
    "        # TODO: Initialize the list of weights matrices, then store\n",
    "        # the network architecture and learning rate\n",
    "        self.alpha = alpha\n",
    "        self.numInputs = numInputs\n",
    "        self.numHiddenLayers = numHiddenLayers\n",
    "        self.numNeuronsPerLayer = numNeuronsPerLayer\n",
    "        self.numOutputs = numOutputs\n",
    "        \n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        for i in range(numHiddenLayers+1):\n",
    "            if i == 0:\n",
    "                self.weights.append(np.random.randn(numInputs, numNeuronsPerLayer))\n",
    "            elif i < numHiddenLayers:\n",
    "                self.weights.append(np.random.randn(numNeuronsPerLayer, numNeuronsPerLayer))\n",
    "            else:\n",
    "                self.weights.append(np.random.randn(numNeuronsPerLayer, numOutputs))\n",
    "        \n",
    "        for i in range(numHiddenLayers+1):\n",
    "            if i >= 0:\n",
    "                self.biases.append(numNeuronsPerLayer)\n",
    "            else:\n",
    "                self.biases.append(numOutputs)\n",
    "        \n",
    "    def __repr__(self): \n",
    "        # construct and return a string that represents the network \n",
    "        # architecture \n",
    "        return \"NeuralNetwork: {}\".format( \"-\".join(str(l) for l in self.layers))\n",
    "\n",
    "    def softmax(X):  \n",
    "        # applies the softmax function to a set of values\n",
    "        expX = np.exp(X)\n",
    "        return expX / expX.sum(axis=1, keepdims=True)\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        # the sigmoid for a given input value\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_deriv(self, x):\n",
    "        # the derivative of the sigmoid\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def predict(self, inputs):\n",
    "        # TODO: Define the predict function\n",
    "        predictions = []\n",
    "        level0 = self.sigmoid(np.dot(inputs, self.weights[0])+ self.biases[0])\n",
    "        predictions.append(level0)\n",
    "        \n",
    "        # hidden layers\n",
    "        for i in range(1, self.numHiddenLayers):\n",
    "            level = self.sigmoid(np.dot(predictions[i-1], self.weights[i]) + self.biases[i])\n",
    "            predictions.append(level)\n",
    "        # last level\n",
    "        if self.numOutputs == 1:\n",
    "            last_level = self.sigmoid(np.dot(inputs, self.weights[-1])+ self.biases[-1])\n",
    "            predictions.append(last_level)\n",
    "        else:\n",
    "            a = np.dot(predictions[-1], self.weights[-1]) + self.biases[-1]\n",
    "            # Apply softmax\n",
    "            expX = np.exp(a)\n",
    "            res = expX / expX.sum(axis=1, keepdims=True)\n",
    "            predictions.append(res)\n",
    "        return predictions\n",
    "        \n",
    "    def train(self, inputs, labels, epochs = 1000, displayUpdate = 100):\n",
    "        # TODO: Define the training step for the network. It should include the forward and back propagation\n",
    "        # steps, the updating of the weights, and it should print the error every 'displayUpdate' epochs\n",
    "        # It must return the errors so that they can be displayed with matplotlib\n",
    "        errors = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Feed Forward\n",
    "            predictions = self.predict(inputs)\n",
    "            \n",
    "            # Error\n",
    "#             print('labels: ',labels.shape,'predictions',predictions[-1].shape)\n",
    "            error = labels - predictions[-1]\n",
    "            errors.append(np.mean(np.abs(error)))\n",
    "            \n",
    "            # print error after X steps\n",
    "            if epoch%100 == 0:\n",
    "                print(\"Error:\", np.mean(np.abs(error)))\n",
    "            \n",
    "            # Backpropagation\n",
    "            deltas = []\n",
    "            bias_deltas = []\n",
    "            \n",
    "            delta = error * self.sigmoid_deriv(predictions[-1])\n",
    "            bias_delta = np.sum(delta)\n",
    "            \n",
    "            deltas.append(delta)\n",
    "            bias_deltas.append(bias_delta)\n",
    "            \n",
    "            for i in range(1,self.numHiddenLayers+1):\n",
    "#                 print('delta: ',delta.shape,' self.weights: ', self.weights[-i].T.shape)\n",
    "                level_error = np.dot(delta, self.weights[-i].T)\n",
    "                delta = level_error * self.sigmoid_deriv(predictions[(i+1)*(-1)])\n",
    "                bias_delta = np.sum(delta)\n",
    "                deltas.append(delta)\n",
    "                bias_deltas.append(bias_delta)\n",
    "            \n",
    "#             for i in deltas:\n",
    "#                 print(delta shape: ', i.shape)\n",
    "\n",
    "            for i in range(1,self.numHiddenLayers + 1):\n",
    "#                 print('dot product of: ', predictions[(i+1)*(-1)].T.shape, ' and ', deltas[i-1].shape)\n",
    "                self.weights[-i] += np.dot(predictions[(i+1)*(-1)].T, deltas[i-1]) * self.alpha\n",
    "                self.biases[-i] += deltas[i-1] * self.alpha\n",
    "            \n",
    "            \n",
    "            \n",
    "                \n",
    "            \n",
    "        return errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "# input dataset\n",
    "XOR_inputs = np.array([  \n",
    "                [0,0],\n",
    "                [0,1],\n",
    "                [1,0],\n",
    "                [1,1]\n",
    "            ])\n",
    "\n",
    "# labels dataset            \n",
    "XOR_labels = np.array([[0,1,1,0]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Error: 0.5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2,4) (4,4) (2,4) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-0ad08f2c4f77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXOR_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXOR_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-df19d0b9bb4e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, inputs, labels, epochs, displayUpdate)\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumHiddenLayers\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;31m#                 print('dot product of: ', predictions[(i+1)*(-1)].T.shape, ' and ', deltas[i-1].shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,4) (4,4) (2,4) "
     ]
    }
   ],
   "source": [
    "#TODO: Test the class with the XOR data\n",
    "print(XOR_inputs.shape[1])\n",
    "nn = NeuralNetwork(2,1,2,4,5)\n",
    "\n",
    "errors = nn.train(XOR_inputs, XOR_labels)\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Error')\n",
    "ax.plot(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the data points for each class\n",
    "class_1 = np.random.randn(700, 2) + np.array([0, -3])  \n",
    "class_2 = np.random.randn(700, 2) + np.array([3, 3])  \n",
    "class_3 = np.random.randn(700, 2) + np.array([-3, 3])\n",
    "\n",
    "feature_set = np.vstack([class_1, class_2, class_3])\n",
    "\n",
    "labels = np.array([0]*700 + [1]*700 + [2]*700)\n",
    "\n",
    "one_hot_labels = np.zeros((2100, 3))\n",
    "\n",
    "for i in range(2100):  \n",
    "    one_hot_labels[i, labels[i]] = 1\n",
    "\n",
    "plt.figure(figsize=(10,10))  \n",
    "plt.scatter(feature_set[:,0], feature_set[:,1], c=labels, s=30, alpha=0.5)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Test the class with the multiple classes data\n",
    "nn = NeuralNetwork(2,1,3,3,0.01)\n",
    "\n",
    "errors = nn.train(feature_set, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On the mnist data set\n",
    "\n",
    "---\n",
    "Train the network to classify hand drawn digits.\n",
    "\n",
    "For this data set, if the training step is taking too long, you can try to adjust the architecture of the network to have fewer layers, or you could try to train it with fewer input. The data has already been loaded and preprocesed so that it can be used with the network.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train and test data from the mnist data set\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Plot a sample data point\n",
    "plt.title(\"Label: \" + str(train_labels[0]))\n",
    "plt.imshow(train_images[0], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "\n",
    "# Flatten the images\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "# turn values from 0-255 to 0-1\n",
    "train_images = train_images.astype('float32') / 255 \n",
    "\n",
    "test_images = test_images.reshape((10000, 28 * 28)) \n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "# Create one hot encoding for the labels\n",
    "train_labels = to_categorical(train_labels) \n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test the class with the mnist data. Test the training of the network with the test_images data, and \n",
    "# record the accuracy of the classification.\n",
    "nn = NeuralNetwork(784, 2, 32, 10, 0.001)\n",
    "\n",
    "errors = nn.train(train_images[0:5000], train_labels[0:5000])\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Error')\n",
    "ax.plot(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After predicting on the *test_images*, use matplotlib to display some of the images that were not correctly classified. Then, answer the following questions: \n",
    "\n",
    "1. **Why do you think those were incorrectly classified?**\n",
    "2. **What could you try doing to improve the classification accuracy?**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
